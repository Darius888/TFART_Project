{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fourth Coursework (Group X)\n",
    "### Theoretical Foundations and Research Topics in Machine Learning\n",
    "\n",
    "Follow the instructions in this notebook. Please remember to upload the filled in jupyter notebook as part of your final submission together with the PDF of the other tasks. It might be a good idea to also upload a PDF/HTML version of your jupyter notebook as this ensures that nothing gets lost during upload.\n",
    "\n",
    "**IMPORTANT:** You are not allowed to use additional imports, i.e., you should implement all functionalities using NumPy only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packages\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import sklearn.datasets as dt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Display figure in the notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activation Functions & Loss (0.5 points)\n",
    "\n",
    "In this section, you will have to implement three different activation functions (_Sigmoid_, _Tanh_, and _ReLU_). Please note that the method _forward()_ is the basic function, while the method _backward()_ should be used for the derivative of the function.\n",
    "\n",
    "Additionally, you will have to implement the _Mean Squared Error_ loss function, which is defined as follows:\n",
    "\n",
    "$$ MSE = \\frac{1}{N} \\sum^{N}_{i=1} (y_i - \\hat{y_i})^2 $$.\n",
    "\n",
    "You can use any functionality that is part of NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid():\n",
    "    def forward(self, x):\n",
    "        # Implement the sigmoid function\n",
    "        ##### YOUR CODE HERE #####\n",
    "        \n",
    "        return 1 / (1 + np.exp(-x))\n",
    "        \n",
    "\n",
    "    def backward(self, x):\n",
    "        # Implement the derivative of the sigmoid function\n",
    "        ##### YOUR CODE HERE #####\n",
    "        \n",
    "        # derivative is df = f * (1 - f)\n",
    "        return (1 / (1 + np.exp(-x))) * (1 - (1 / (1 + np.exp(-x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TanH():\n",
    "    def forward(self, x):\n",
    "        # Implement the tanh function\n",
    "        ##### YOUR CODE HERE #####\n",
    "        \n",
    "        return (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))\n",
    "\n",
    "    def backward(self, x):\n",
    "        # Implement the derivative of the tanh function\n",
    "        ##### YOUR CODE HERE #####\n",
    "        \n",
    "        tanh = (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))\n",
    "        return 1 - tanh**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU():\n",
    "    def forward(self, x):\n",
    "        # Implement the relu function\n",
    "        ##### YOUR CODE HERE #####\n",
    "        \n",
    "        return x * (x > 0)\n",
    "\n",
    "    def backward(self, x):\n",
    "        # Implement the derivative of the relu function\n",
    "        ##### YOUR CODE HERE #####\n",
    "    \n",
    "        return 1. * (x > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSE():\n",
    "    def forward(self, y_pred, y_true):\n",
    "        # Implement the mse function\n",
    "        ##### YOUR CODE HERE #####\n",
    "        \n",
    "        return (np.square(y_pred - y_true)).mean()\n",
    "\n",
    "    def backward(self, y_pred, y_true):\n",
    "        # Implement the derivative of the mse function\n",
    "        ##### YOUR CODE HERE #####\n",
    "        \n",
    "        return 2 * (y_pred - y_true).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-Layer Perceptron (2 points)\n",
    "\n",
    "In this section, you will have to implement your very own _Multi-Layer Perceptron_. For the network architecture, we will consider only one hidden layer and no activation function for the output neuron (i.e. apply the activation function to the hidden layer but not to the output layer).\n",
    "\n",
    "_Hint: You will need to compute the derivatives $\\frac{\\partial L}{\\partial W_o}, \\frac{\\partial L}{\\partial b_o}, \\frac{\\partial L}{\\partial W_h}$ and $\\frac{\\partial L}{\\partial b_h}$ (using the chain rule) as they are required for the backward pass._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP():\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, activation_func = TanH()):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        W_h: \n",
    "            weight matrix from input layer to hidden layer with size (input_size, hidden_size)\n",
    "        b_h: \n",
    "            bias vector for the hidden layer with size (hidden_size)\n",
    "        W_o: \n",
    "            weight matrix from hidden layer to output layer with size (hidden_size, output_size)\n",
    "        b_o: \n",
    "            bias vector for the hidden layer with size (output_size)\n",
    "        activation_func: \n",
    "            activation function of your choice\n",
    "        \"\"\"\n",
    "        ##### YOUR CODE HERE #####\n",
    "        \n",
    "        self.W_h = np.random.random((hidden_size, input_size))\n",
    "        self.b_h = np.random.random((hidden_size, 1))\n",
    "        \n",
    "        self.W_o = np.random.random((output_size, hidden_size))\n",
    "        self.b_o = np.random.random((output_size, 1))\n",
    "        \n",
    "        self.activation_func = activation_func\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        forward pass of the MLP\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x:\n",
    "            input vector of size (input_size)\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        y:\n",
    "            output vector of size (output_size)\n",
    "        \"\"\"\n",
    "        ##### YOUR CODE HERE #####\n",
    "        \n",
    "        # first weights layer (hidden) (multiply + sum)\n",
    "        f = np.dot(self.W_h, x) + self.b_h\n",
    "        \n",
    "        # activation function for the hidden layer\n",
    "        f = self.activation_func.forward(f)\n",
    "        \n",
    "        # output weights layer (multiply + sum)\n",
    "        y = np.dot(self.W_o, f) + self.b_o\n",
    "        \n",
    "        return y\n",
    "        \n",
    "    def forward_(self, x):\n",
    "        \"\"\"\n",
    "        forward pass of the MLP with additional return values\n",
    "        \n",
    "        Paramaters\n",
    "        ----------\n",
    "        x:\n",
    "            input vector of size (input_size)\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        y:\n",
    "            output vector of size (output_size)\n",
    "        h:\n",
    "            activation of the hidden layer of size (hidden_size)\n",
    "        z_h:\n",
    "            pre-activation of the hidden layer of size (hidden_size)\n",
    "            i.e., the input vector to the activation function\n",
    "        \"\"\"\n",
    "        ##### YOUR CODE HERE #####\n",
    "        \n",
    "        # pre-activation of the hidden layer\n",
    "        z_h = np.dot(self.W_h, x) + self.b_h    \n",
    "        \n",
    "        # activation of the hidden layer\n",
    "        h = self.activation_func.forward(z_h)\n",
    "                \n",
    "        # output vector\n",
    "        y = np.dot(self.W_o, h) + self.b_o\n",
    "        \n",
    "        return y, h, z_h\n",
    "    \n",
    "    def backward(self, x, h, z_h, dloss):\n",
    "        \"\"\"\n",
    "        backward pass of the MLP\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x:\n",
    "            input vector of size (input_size)\n",
    "        h:\n",
    "            activation of the hidden layer of size (hidden_size)\n",
    "        z_h:\n",
    "            pre-activation of the hidden layer of size (hidden_size)\n",
    "            i.e., the input vector to the activation function\n",
    "        dloss:\n",
    "            gradient of the loss function with respect to y_pred\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        grads:\n",
    "            dictionary containing the elements\n",
    "            - W_h: gradients for W_h\n",
    "            - b_h: gradients for b_h\n",
    "            - W_o: gradients for W_o\n",
    "            - b_o: gradients for b_o\n",
    "        \"\"\"\n",
    "        ##### YOUR CODE HERE #####\n",
    "        \n",
    "        n = x.shape[1]\n",
    "\n",
    "        dW_o = 1./n * np.dot(dloss, h.T)\n",
    "        db_o = 1./n * np.sum(dloss)\n",
    "        dloss1 = np.dot(self.W_o.T, dloss) * (1 - np.power(h, 2))\n",
    "        \n",
    "        dW_h = 1./n * np.dot(dloss1, x.T)\n",
    "        db_h = 1./n * np.sum(dloss1)\n",
    "        \n",
    "        grads = {\"W_h\": dW_h,\n",
    "                \"b_h\": db_h,\n",
    "                \"W_o\": dW_o,\n",
    "                \"b_o\": db_o}\n",
    "        \n",
    "        return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Descent (2 points)\n",
    "\n",
    "In this section, you will have to implement the training algorithm using _Gradient Descent_. \n",
    "\n",
    "While we provide you with the wrapper function, you need to implement the methods _evaluate()_ and _update()_, where the computation of the gradients and the weight update should be performed as part of the _update()_ method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data, model, loss_func):\n",
    "    \"\"\"\n",
    "    function to evaluate the test data\n",
    "    i.e., just forward pass and loss computation\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data:\n",
    "        input data containing X and y\n",
    "    model:\n",
    "        the initialized MLP model\n",
    "    loss_func:\n",
    "        loss function of your choice\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    losses:\n",
    "        array containing all individual losses\n",
    "        i.e., for each data sample\n",
    "    \"\"\"\n",
    "    ##### YOUR CODE HERE #####\n",
    "        \n",
    "    X = np.array((list(zip(*data)))[0])\n",
    "    y = np.array((list(zip(*data)))[1])\n",
    "    \n",
    "    fwd = model.forward_(X.T)\n",
    "    \n",
    "    \n",
    "    losses = loss_func.forward(fwd[0], y)\n",
    "    \n",
    "    return losses\n",
    "    \n",
    "def update(data, model, loss_func, learning_rate):\n",
    "    \"\"\"\n",
    "    function to calculate gradients and perform weight updates\n",
    "    i.e., forward pass + loss computation + backward pass + weight update\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data:\n",
    "        input data containing X and y\n",
    "    model:\n",
    "        the initialized MLP model\n",
    "    loss_func:\n",
    "        loss function of your choice\n",
    "    learning_rate:\n",
    "        float value defining the learning rate\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    losses:\n",
    "        array containing all individual losses\n",
    "        i.e., for each data sample\n",
    "    \"\"\"\n",
    "    ##### YOUR CODE HERE #####\n",
    "            \n",
    "    X = np.array((list(zip(*data)))[0])\n",
    "    y = np.array((list(zip(*data)))[1])\n",
    "    \n",
    "    y = y.reshape((y.shape[0], 1))\n",
    "    \n",
    "    # forward pass\n",
    "    fwd = model.forward_(X.T)\n",
    "    \n",
    "    # compute losses\n",
    "    losses = loss_func.forward(fwd[0], y)\n",
    "    \n",
    "    # backward pass\n",
    "    gradients = model.backward(X.T, fwd[1], fwd[2], losses)\n",
    "    \n",
    "    # weight updates\n",
    "    model.W_h = model.W_h - gradients[\"W_h\"]*learning_rate\n",
    "    model.b_h = model.b_h - gradients[\"b_h\"]*learning_rate\n",
    "    model.W_o = model.W_o - gradients[\"W_o\"]*learning_rate\n",
    "    model.b_o = model.b_o - gradients[\"b_o\"]*learning_rate\n",
    "    \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform gradient descent, no ToDo for you\n",
    "def gradient_descent(train_data, test_data, model, loss_func, epochs, learning_rate):\n",
    "    valid_losses = evaluate(test_data, model, loss_func)\n",
    "    print(\"Initial Validation: \" + str(np.mean(valid_losses)))\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        train_losses = update(train_data, model, loss_func, learning_rate)\n",
    "        valid_losses = evaluate(test_data, model, loss_func)\n",
    "        print(\"Epoch \" + str(epoch) + \": \" + str(np.mean(train_losses)) + \" Train Loss, \" + str(np.mean(valid_losses)) + \" Valid Loss\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train your model (0.5 points)\n",
    "\n",
    "In this section, you will have to initialise your MLP (using defined hyperparameters) and train it on the provided data. For the training, you can, of course, simply use the _gradient$\\_$descent()_ method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating toy data, no ToDo for you\n",
    "X, y = dt.make_regression(n_samples = 1000, n_features = 20)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "train_data = list(zip(X_train, y_train))\n",
    "test_data = list(zip(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Validation: 17586.958341370075\n",
      "Epoch 0: 26066.110447719722 Train Loss, 19212.142701386183 Valid Loss\n",
      "Epoch 1: 25944.23526597317 Train Loss, 19255.52878878995 Valid Loss\n",
      "Epoch 2: 25994.85008302487 Train Loss, 19302.545768502536 Valid Loss\n",
      "Epoch 3: 26049.109894972153 Train Loss, 19353.221715849595 Valid Loss\n",
      "Epoch 4: 26107.043792729743 Train Loss, 19407.586808232954 Valid Loss\n",
      "Epoch 5: 26168.68297739408 Train Loss, 19465.673360693265 Valid Loss\n",
      "Epoch 6: 26234.060796393835 Train Loss, 19527.515864152778 Valid Loss\n",
      "Epoch 7: 26303.21278233067 Train Loss, 19593.15102641595 Valid Loss\n",
      "Epoch 8: 26376.176694588827 Train Loss, 19662.617816011865 Valid Loss\n",
      "Epoch 9: 26452.99256379787 Train Loss, 19735.957508968604 Valid Loss\n",
      "Epoch 10: 26533.7027392399 Train Loss, 19813.213738616418 Valid Loss\n",
      "Epoch 11: 26618.35193929883 Train Loss, 19894.432548523637 Valid Loss\n",
      "Epoch 12: 26706.98730505633 Train Loss, 19979.662448676376 Valid Loss\n",
      "Epoch 13: 26799.65845714651 Train Loss, 20068.95447502074 Valid Loss\n",
      "Epoch 14: 26896.41755598917 Train Loss, 20162.362252494517 Valid Loss\n",
      "Epoch 15: 26997.3193655289 Train Loss, 20259.942061683403 Valid Loss\n",
      "Epoch 16: 27102.421320616926 Train Loss, 20361.752909246177 Valid Loss\n",
      "Epoch 17: 27211.78359818033 Train Loss, 20467.8566022622 Valid Loss\n",
      "Epoch 18: 27325.46919233364 Train Loss, 20578.317826664887 Valid Loss\n",
      "Epoch 19: 27443.543993597203 Train Loss, 20693.204229934938 Valid Loss\n"
     ]
    }
   ],
   "source": [
    "# Initialize and train the model\n",
    "\n",
    "model = MLP(20, 40, 1)\n",
    "loss_func = MSE()\n",
    "epochs = 20\n",
    "learning_rate = 0.001\n",
    "\n",
    "gradient_descent(train_data, test_data, model, loss_func, epochs, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
