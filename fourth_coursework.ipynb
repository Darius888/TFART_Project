{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fourth Coursework (Group X)\n",
    "### Theoretical Foundations and Research Topics in Machine Learning\n",
    "\n",
    "Follow the instructions in this notebook. Please remember to upload the filled in jupyter notebook as part of your final submission together with the PDF of the other tasks. It might be a good idea to also upload a PDF/HTML version of your jupyter notebook as this ensures that nothing gets lost during upload.\n",
    "\n",
    "**IMPORTANT:** You are not allowed to use additional imports, i.e., you should implement all functionalities using NumPy only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packages\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import sklearn.datasets as dt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Display figure in the notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activation Functions & Loss (0.5 points)\n",
    "\n",
    "In this section, you will have to implement three different activation functions (_Sigmoid_, _Tanh_, and _ReLU_). Please note that the method _forward()_ is the basic function, while the method _backward()_ should be used for the derivative of the function.\n",
    "\n",
    "Additionally, you will have to implement the _Mean Squared Error_ loss function, which is defined as follows:\n",
    "\n",
    "$$ MSE = \\frac{1}{N} \\sum^{N}_{i=1} (y_i - \\hat{y_i})^2 $$.\n",
    "\n",
    "You can use any functionality that is part of NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid():\n",
    "    def forward(self, x):\n",
    "        # Implement the sigmoid function\n",
    "        ##### YOUR CODE HERE #####\n",
    "        \n",
    "        return 1 / (1 + np.exp(-x))\n",
    "        \n",
    "\n",
    "    def backward(self, x):\n",
    "        # Implement the derivative of the sigmoid function\n",
    "        ##### YOUR CODE HERE #####\n",
    "        \n",
    "        # derivative is df = f * (1 - f)\n",
    "        return (1 / (1 + np.exp(-x))) * (1 - (1 / (1 + np.exp(-x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TanH():\n",
    "    def forward(self, x):\n",
    "        # Implement the tanh function\n",
    "        ##### YOUR CODE HERE #####\n",
    "        \n",
    "        return (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))\n",
    "\n",
    "    def backward(self, x):\n",
    "        # Implement the derivative of the tanh function\n",
    "        ##### YOUR CODE HERE #####\n",
    "        \n",
    "        tanh = (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))\n",
    "        return 1 - tanh**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU():\n",
    "    def forward(self, x):\n",
    "        # Implement the relu function\n",
    "        ##### YOUR CODE HERE #####\n",
    "        \n",
    "        return x * (x > 0)\n",
    "\n",
    "    def backward(self, x):\n",
    "        # Implement the derivative of the relu function\n",
    "        ##### YOUR CODE HERE #####\n",
    "    \n",
    "        return 1. * (x > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSE():\n",
    "    def forward(self, y_pred, y_true):\n",
    "        # Implement the mse function\n",
    "        ##### YOUR CODE HERE #####\n",
    "        \n",
    "        return (np.square(y_pred - y_true)).mean()\n",
    "\n",
    "    def backward(self, y_pred, y_true):\n",
    "        # Implement the derivative of the mse function\n",
    "        ##### YOUR CODE HERE #####\n",
    "        \n",
    "        return 2 * (y_pred - y_true).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-Layer Perceptron (2 points)\n",
    "\n",
    "In this section, you will have to implement your very own _Multi-Layer Perceptron_. For the network architecture, we will consider only one hidden layer and no activation function for the output neuron (i.e. apply the activation function to the hidden layer but not to the output layer).\n",
    "\n",
    "_Hint: You will need to compute the derivatives $\\frac{\\partial L}{\\partial W_o}, \\frac{\\partial L}{\\partial b_o}, \\frac{\\partial L}{\\partial W_h}$ and $\\frac{\\partial L}{\\partial b_h}$ (using the chain rule) as they are required for the backward pass._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP():\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, activation_func = ReLU()):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        W_h: \n",
    "            weight matrix from input layer to hidden layer with size (input_size, hidden_size)\n",
    "        b_h: \n",
    "            bias vector for the hidden layer with size (hidden_size)\n",
    "        W_o: \n",
    "            weight matrix from hidden layer to output layer with size (hidden_size, output_size)\n",
    "        b_o: \n",
    "            bias vector for the hidden layer with size (output_size)\n",
    "        activation_func: \n",
    "            activation function of your choice\n",
    "        \"\"\"\n",
    "        ##### YOUR CODE HERE #####\n",
    "        \n",
    "        self.W_h = np.random.random((input_size, hidden_size))\n",
    "        self.b_h = np.random.random((1, hidden_size))\n",
    "        \n",
    "        self.W_o = np.random.random((hidden_size, output_size))\n",
    "        self.b_o = np.random.random((1, output_size))\n",
    "        \n",
    "        self.activation_func = activation_func\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        forward pass of the MLP\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x:\n",
    "            input vector of size (input_size)\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        y:\n",
    "            output vector of size (output_size)\n",
    "        \"\"\"\n",
    "        ##### YOUR CODE HERE #####\n",
    "        \n",
    "        # first weights layer (hidden) (multiply + sum)\n",
    "        f = np.dot(x, self.W_h) + self.b_h\n",
    "        \n",
    "        # activation function for the hidden layer\n",
    "        f = self.activation_func.forward(f)\n",
    "        \n",
    "        # output weights layer (multiply + sum)\n",
    "        y = np.dot(f, self.W_o) + self.b_o\n",
    "        \n",
    "        return y\n",
    "        \n",
    "    def forward_(self, x):\n",
    "        \"\"\"\n",
    "        forward pass of the MLP with additional return values\n",
    "        \n",
    "        Paramaters\n",
    "        ----------\n",
    "        x:\n",
    "            input vector of size (input_size)\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        y:\n",
    "            output vector of size (output_size)\n",
    "        h:\n",
    "            activation of the hidden layer of size (hidden_size)\n",
    "        z_h:\n",
    "            pre-activation of the hidden layer of size (hidden_size)\n",
    "            i.e., the input vector to the activation function\n",
    "        \"\"\"\n",
    "        ##### YOUR CODE HERE #####\n",
    "        \n",
    "        # pre-activation of the hidden layer\n",
    "        z_h = np.dot(x, self.W_h) + self.b_h    \n",
    "        \n",
    "        # activation of the hidden layer\n",
    "        h = self.activation_func.forward(z_h)\n",
    "                \n",
    "        print(h)\n",
    "        # output vector\n",
    "        y = np.dot(h, self.W_o) + self.b_o\n",
    "        \n",
    "        return y, h, z_h\n",
    "    \n",
    "    def backward(self, x, h, z_h, dloss):\n",
    "        \"\"\"\n",
    "        backward pass of the MLP\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x:\n",
    "            input vector of size (input_size)\n",
    "        h:\n",
    "            activation of the hidden layer of size (hidden_size)\n",
    "        z_h:\n",
    "            pre-activation of the hidden layer of size (hidden_size)\n",
    "            i.e., the input vector to the activation function\n",
    "        dloss:\n",
    "            gradient of the loss function with respect to y_pred\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        grads:\n",
    "            dictionary containing the elements\n",
    "            - W_h: gradients for W_h\n",
    "            - b_h: gradients for b_h\n",
    "            - W_o: gradients for W_o\n",
    "            - b_o: gradients for b_o\n",
    "        \"\"\"\n",
    "        ##### YOUR CODE HERE #####\n",
    "        \n",
    "        n = x.shape[0]\n",
    "        print(h.T.shape)\n",
    "        print(len(dloss))\n",
    "\n",
    "        dW_o = 1./n * np.dot(h.T, dloss)\n",
    "        db_o = 1./n * np.sum(dloss, axis = 1, keepdims = True)\n",
    "        dloss1 = np.dot(dloss, self.W_o.T) * (1 - np.power(h, 2))\n",
    "        \n",
    "        dW_h = 1./n * np.dot(x.T, dloss1)\n",
    "        db_h = 1./n * np.sum(dloss1, axis = 1, keepdims = True)\n",
    "        \n",
    "        grads = {\"W_h\": dW_h,\n",
    "                \"b_h\": db_h,\n",
    "                \"W_o\": dW_o,\n",
    "                \"b_o\": db_o}\n",
    "        \n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLP(2, 3, 5, activation_func=ReLU())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.3765608 , 0.00212064],\n",
       "       [0.54238187, 0.12584133]])"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.random.random((2, 2))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.13654587 0.91772233 0.82905093]\n",
      " [1.32447673 0.96372966 0.88182197]]\n"
     ]
    }
   ],
   "source": [
    "fwd = mlp.forward_(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 2)\n",
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'W_h': array([[-0.2527643 ,  0.06230835,  0.12002154],\n",
       "        [-0.04653189,  0.00590861,  0.01442553]]),\n",
       " 'b_h': array([[ 0.11561423],\n",
       "        [-0.21012907]]),\n",
       " 'W_o': array([[0.61525565, 0.24610226, 0.61525565, 0.24610226, 0.49220452],\n",
       "        [0.470363  , 0.1881452 , 0.470363  , 0.1881452 , 0.3762904 ],\n",
       "        [0.42771823, 0.17108729, 0.42771823, 0.17108729, 0.34217458]]),\n",
       " 'b_o': array([[0.9],\n",
       "        [0.9]])}"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.backward(x, fwd[1], fwd[2], [[0.5, 0.2, 0.5, 0.2, 0.4], [0.5, 0.2, 0.5, 0.2, 0.4]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Descent (2 points)\n",
    "\n",
    "In this section, you will have to implement the training algorithm using _Gradient Descent_. \n",
    "\n",
    "While we provide you with the wrapper function, you need to implement the methods _evaluate()_ and _update()_, where the computation of the gradients and the weight update should be performed as part of the _update()_ method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data, model, loss_func):\n",
    "    \"\"\"\n",
    "    function to evaluate the test data\n",
    "    i.e., just forward pass and loss computation\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data:\n",
    "        input data containing X and y\n",
    "    model:\n",
    "        the initialized MLP model\n",
    "    loss_func:\n",
    "        loss function of your choice\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    losses:\n",
    "        array containing all individual losses\n",
    "        i.e., for each data sample\n",
    "    \"\"\"\n",
    "    ##### YOUR CODE HERE #####\n",
    "        \n",
    "    \n",
    "    \n",
    "def update(data, model, loss_func, learning_rate):\n",
    "    \"\"\"\n",
    "    function to calculate gradients and perform weight updates\n",
    "    i.e., forward pass + loss computation + backward pass + weight update\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data:\n",
    "        input data containing X and y\n",
    "    model:\n",
    "        the initialized MLP model\n",
    "    loss_func:\n",
    "        loss function of your choice\n",
    "    learning_rate:\n",
    "        float value defining the learning rate\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    losses:\n",
    "        array containing all individual losses\n",
    "        i.e., for each data sample\n",
    "    \"\"\"\n",
    "    ##### YOUR CODE HERE #####\n",
    "        \n",
    "    ##########################\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform gradient descent, no ToDo for you\n",
    "def gradient_descent(train_data, test_data, model, loss_func, epochs, learning_rate):\n",
    "    valid_losses = evaluate(test_data, model, loss_func)\n",
    "    print(\"Initial Validation: \" + str(np.mean(valid_losses)))\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        train_losses = update(train_data, model, loss_func, learning_rate)\n",
    "        valid_losses = evaluate(test_data, model, loss_func)\n",
    "        \n",
    "        print(\"Epoch \" + str(epoch) + \": \" + str(np.mean(train_losses)) + \" Train Loss, \" + str(np.mean(valid_losses)) + \" Valid Loss\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train your model (0.5 points)\n",
    "\n",
    "In this section, you will have to initialise your MLP (using defined hyperparameters) and train it on the provided data. For the training, you can, of course, simply use the _gradient$\\_$descent()_ method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating toy data, no ToDo for you\n",
    "X, y = dt.make_regression(n_samples = 1000, n_features = 20)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "train_data = list(zip(X_train, y_train))\n",
    "test_data = list(zip(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train the model\n",
    "\n",
    "##### YOUR CODE HERE #####\n",
    "\n",
    "##########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 20)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
